#######################################################################################################
# This script is used to generate a list of metadata jsons for documents crawled and deposited by a
# given spider in Gamechanger's S3 bucket:
#   - s3://advana-data-zone/bronze/gamechanger/pdf/
#
# The script is intended to run locally against metadata files downloaded from the s3 bucket. Download
# In the terminal, navigate to the folder where the metadata documents are to be downloaded and run:
#   - aws s3 cp s3://advana-data-zone/bronze/gamechanger/pdf/ . --exclude="*" --include="*.metadata" \
#       --recursive &> /dev/null&
#
# The crawler who's documents are to be targeted must be defined in the main block of this script. A target directory
# path, as well as an output directory path, can also be defined, or the os.getcwd() default will assign
# the current directory. The resulting compiled json can then be used to delete the documents included
# from the full Gamechanger corpus.
#######################################################################################################

import os
import json
from pathlib import Path
import boto3

from configuration.utils import get_connection_helper_from_env

def query_uncat_es(ch, index):
    body = {
        "query": {
            "bool": {
                "must" : [
                {
                    "term": {
                        "crawler_used_s": "air_force_pubs"
                    }
                },
                {
                    "term": {
                        "display_doc_type_s": "Uncategorized"
                    }
                },
                {
                    "term": {
                        "is_revoked_b": False
                    }
                }
            ]
        }
    },
    "size": 10000,
    "fields": ["id", "original_ingest_date", "current_ingest_date", "display_org_s", "filename"],
    "_source": False
    }
    response = ch.es_client.search(index=index, body=body)
    return response

def filter_es_response(response):
    fnames = []
    prefix = "bronze/gamechanger/pdf/"
    for hit in response['hits']['hits']:
        full_path = f"{prefix + hit['fields']['filename'][0]}.metadata"
        fnames.append(full_path)
    return fnames

def download_and_append_to_json(fnames_list, outfile):
    s3 = boto3.client('s3')
    bucket_name = "advana-data-zone"
    temp_prefix = '/home/gamechanger/de_test_scripts/temp'

    json_data_list = []
    for fname in fnames_list:
        try:
            temp_file_path = f"{temp_prefix}/temp_{fname.replace('/', '_')}"
            s3.download_file(bucket_name, fname, temp_file_path)
            with open(temp_file_path, 'r') as temp_file:
                json_data_list.append(json.load(temp_file))
        except:
            continue

        os.remove(temp_file_path)

    print(f"total: {len(fnames_list)}, actual downloaded: {len(json_data_list)}")
    with open(outfile, 'x') as f:
        for json_data in json_data_list:
            f.write(json.dumps(json_data))
            f.write('\n')

    print(f"data written to {outfile}!")

def get_current_gc_alias(ch):
    es_response = ch.es_client.cat.aliases()
    return list(filter(lambda x: (x != ''), [line for line in es_response.split('\n') if line.startswith('gamechanger ')][0].split(' ')))[1]

def get_target_jsons(crawler, dir):
    """generate json file containing jsons compiled from metadata generated by a given crawler"""
    files = [file for file in dir.glob("*.metadata")]
    lines = []
    for filename in files:
        with open(filename, 'r') as f:
            data = json.load(f)
            if data['crawler_used'] == crawler:
                data['filename'] = Path(filename).stem
                lines.append(data)
    return lines

def write_output_json(lines, output_file, result_dir):
    with open(os.path.join(result_dir, output_file), mode='a') as new_file:
        cnt = 0
        for line in lines:
            jsondoc = json.dumps(line) + '\n'
            new_file.write(jsondoc)
            cnt += 1
    return "Metadata from " + str(cnt) + " documents compiled in " + output_file


def main():
    crawler = "dha_pubs" # Crawler to get metadata jsons from
    dir = Path(os.getcwd(), "metadata") # Directory path of metadata jsons for the full GC corpus
    output_file = "test_output.json" # Filename for the compiled json resulting from this script
    result_dir = os.getcwd() # Directory path where the output json is to be written
    lines = get_target_jsons(crawler, dir)
    result_txt = write_output_json(lines, output_file, result_dir)
    print(result_txt)

if __name__ == "__main__":
    ch = get_connection_helper_from_env()
    current_index_name = get_current_gc_alias(ch)
    #print(filter_es_response(query_uncat_es(ch, current_index_name)))
    download_and_append_to_json(filter_es_response(query_uncat_es(ch, current_index_name)), "/home/gamechanger/de_test_scripts/airforce_delete_111523.json")
